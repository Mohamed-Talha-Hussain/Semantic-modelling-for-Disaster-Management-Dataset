{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2455ad1-4475-4b6d-a463-a09e72ca6ddc",
   "metadata": {},
   "source": [
    "### 1. Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d454185-d2d1-4ad5-b092-88ea9e10654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from string import punctuation\n",
    "import SPARQLWrapper\n",
    "from SPARQLWrapper import SPARQLWrapper2\n",
    "import sys\n",
    "import os\n",
    "import contextlib\n",
    "import stanza\n",
    "import logging\n",
    "from IPython.display import display, Markdown\n",
    "# Suppress output during the NLP pipeline initialization\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n",
    "            yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95295041-653c-4ac7-a6e7-ab08f52b9559",
   "metadata": {},
   "source": [
    "### 2. Defining Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f60672-39fe-41a2-ba68-0d698174c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "def bag_of_words():\n",
    "    class_list = [['eventName', 'disaster'], ['region', 'place'], ['SubGroupName', 'category']]\n",
    "    prop_list = [['TypeName', 'type'], ['startYearValue', 'year', 'date'], ['countryName', 'country', 'gpe'], ['GroupName', 'group'], ['magnitude', 'magnitude'], ['totalAffected', 'affect'], ['totalDeaths', 'death', 'die', 'kill'], ['regionName', 'region', 'loc']]\n",
    "    return class_list, prop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3d293-1482-43f9-86e3-cb8edb268aae",
   "metadata": {},
   "source": [
    "### 3. Defining functions for Cleaning Data (Normalize -> tokenize -> stopword removal -> lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f86dc4-dd12-40c8-b9aa-c5dba3d7b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    text = data\n",
    "    def normalize(text):\n",
    "        processed_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "        processed_text = \" \". join(processed_text. split())\n",
    "        return processed_text\n",
    "    \n",
    "    def strip_stopwords(tokens):\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        clean_words = []\n",
    "        for word in tokens:\n",
    "            if word not in stop_words:\n",
    "                clean_words.append(word)\n",
    "        #print(clean_words)\n",
    "        return clean_words\n",
    "    def lemmatize(text):\n",
    "        text = normalize(text)\n",
    "        lem = WordNetLemmatizer()\n",
    "        lems = []\n",
    "        tokens = word_tokenize(text)\n",
    "        #print(tokens)\n",
    "        clean_tokens = strip_stopwords(tokens)\n",
    "        for token in clean_tokens:\n",
    "            temp = lem.lemmatize(token, 'n')\n",
    "            lems.append(lem.lemmatize(temp, 'v'))\n",
    "        return text, tokens, clean_tokens, lems\n",
    "    text, tokens, clean_tokens, lemmatized_words = lemmatize(text)\n",
    "    return text, tokens, clean_tokens, lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff017f86-5ddd-4a09-996c-d9fb731a0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Enter your question all of the disasters and    China also the  maybe\"\n",
    "# clean_data(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad452d8-0df0-4ce2-b55f-5b9e1e08b225",
   "metadata": {},
   "source": [
    "### 4. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458c5e3c-210f-40d9-81a8-138606af9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(new_text, lemmatized_words):\n",
    "    ner_tokens = lemmatized_words\n",
    "    import stanza\n",
    "    # nlp = stanza.Pipeline(\"en\")\n",
    "    logging.getLogger(\"stanza\").setLevel(logging.WARNING)\n",
    "    with suppress_stdout_stderr():\n",
    "        nlp = stanza.Pipeline(\"en\", download_method=stanza.DownloadMethod.NONE)\n",
    "    doc = nlp(new_text)\n",
    "    if hasattr(doc, 'entities') and len(doc.entities) > 0:\n",
    "        # print(doc.entities[0].text)\n",
    "        # print(doc.entities[0].type)\n",
    "        ner_tokens.append(doc.entities[0].type.lower())\n",
    "    #     lemmatized_words.append(doc.entities[0].text)\n",
    "    return ner_tokens, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3979ac-2b18-448b-9ef8-649656cdca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text = ['Enter', 'question', 'disaster', 'China', 'also', 'maybe']\n",
    "# text = \"Enter your question all of the disasters and    China also the  maybe\"\n",
    "# ner(text, clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497476f-a0b6-4ff8-a9a8-e7cd4fcf908c",
   "metadata": {},
   "source": [
    "### 5. Ontology Mapping (lemmatized words -> Bag of Words -> Classes and Properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0605a4-343b-47e7-affb-f6fa0ab6c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ontology_map(lemmatized_words, cls_lst, prp_lst):\n",
    "    class_list, prop_list =  cls_lst, prp_lst\n",
    "    lemmatized_words = lemmatized_words\n",
    "    # Function to map lemmatized words to ontology classes and properties\n",
    "    def map_to_ontology(lemmatized_words, class_list, prop_list):\n",
    "        query_classes = []\n",
    "        query_properties = []\n",
    "        \n",
    "        # Match lemmatized words to classes\n",
    "        for word in lemmatized_words:\n",
    "            for class_group in class_list:\n",
    "                if word.lower() in [cls.lower() for cls in class_group]:\n",
    "                    query_classes.append(class_group[0])  # Select the canonical class (e.g., 'eventName')\n",
    "        \n",
    "        # Match lemmatized words to properties\n",
    "        for word in lemmatized_words:\n",
    "            for prop_group in prop_list:\n",
    "                if word.lower() in [prop.lower() for prop in prop_group]:\n",
    "                    query_properties.append(prop_group[0])  # Select the canonical property (e.g., 'Type')\n",
    "        \n",
    "        return query_classes, query_properties\n",
    "    \n",
    "    # Map lemmatized words to ontology\n",
    "    query_classes, query_properties = map_to_ontology(lemmatized_words, class_list, prop_list)\n",
    "    return query_classes, query_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3758d-a092-49d0-8529-2e07bd73f739",
   "metadata": {},
   "source": [
    "### 6. Query building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90fcd959-81d3-448a-9f12-eb887f26f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_build(query_classes, query_properties, doc):\n",
    "    import SPARQLWrapper\n",
    "    query_classes, query_properties, doc = query_classes, query_properties, doc\n",
    "    # Build the SPARQL query dynamically based on the mapped classes and properties\n",
    "    # Initialize the SPARQL query\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT \"\"\" + \" \".join([f\"?{cls}\" for cls in query_classes] + [f\"?{prop}\" for prop in query_properties]) + \"\"\"\n",
    "    WHERE {\n",
    "      ?event a <http://www.semanticweb.org/mohdtalhahussain/ontologies/2024/10/Disaster_Management_dataset#DisasterEvent>.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add the class and property conditions dynamically\n",
    "    sparql_query += \"\\n\".join([f\"?event <http://www.semanticweb.org/mohdtalhahussain/ontologies/2024/10/Disaster_Management_dataset#{cls}> ?{cls}.\" for cls in query_classes])\n",
    "    sparql_query += \"\\n\".join([f\"?event <http://www.semanticweb.org/mohdtalhahussain/ontologies/2024/10/Disaster_Management_dataset#{prop}> ?{prop}.\" for prop in query_properties])\n",
    "    if hasattr(doc, 'entities') and len(doc.entities) > 0:\n",
    "        if doc.entities[0].type == \"GPE\":\n",
    "            if hasattr(doc, 'entities') and len(doc.entities) > 0:\n",
    "                cvalue = doc.entities[0].text\n",
    "                #print(doc.entities[0].type)\n",
    "                sparql_query += (f\"FILTER(?countryName = \\\"{cvalue}\\\")\")\n",
    "        # Regular expression to match a year (4 digits)\n",
    "        year_pattern = r\"\\b(\\d{4})\\b\"\n",
    "        # Search for a year in the input string\n",
    "        match = re.search(year_pattern, text)\n",
    "        if match:\n",
    "            # Extracted year\n",
    "            yvalue = int(match.group(1))\n",
    "            sparql_query += (f\"FILTER(?startYearValue = \\\"{yvalue}\\\")\")\n",
    "        if doc.entities[0].type == \"LOC\":\n",
    "            lvalue = doc.entities[0].text\n",
    "            sparql_query += (f\"FILTER(?regionName = \\\"{lvalue}\\\")\")\n",
    "    sparql_query += \"\"\"\n",
    "    } LIMIT 20\n",
    "    \"\"\"\n",
    "    return sparql_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ae3de-b008-4bd9-a0e8-391a4988a86a",
   "metadata": {},
   "source": [
    "### 7. Fetching Data from fuseki server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0ddd56-d5f9-41fb-9561-af94cb08afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_fetch(sparql_query):\n",
    "    sparql_query = sparql_query\n",
    "    # Connect to Fuseki server and execute the query\n",
    "    sparql = SPARQLWrapper2(\"http://localhost:3030/Disaster_management_dataset/\")\n",
    "    sparql.setQuery(sparql_query)\n",
    "    data = sparql.query().bindings\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d00c4-89f8-422a-b12d-9b09d3b31b7a",
   "metadata": {},
   "source": [
    "### 8. Creating DataFrame from the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15c6f98-1469-455a-a626-799653db75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data, query_classes, query_properties):\n",
    "    data, query_classes, query_properties = data, query_classes, query_properties\n",
    "    import pandas as pd\n",
    "    events = []\n",
    "    for result in data:\n",
    "        event_data = []\n",
    "        for cls in query_classes:\n",
    "            event_data.append(result[f\"{cls}\"].value if f\"{cls}\" in result else None)\n",
    "        for prop in query_properties:\n",
    "            event_data.append(result[f\"{prop}\"].value if f\"{prop}\" in result else None)\n",
    "        events.append(event_data)\n",
    "    # Dynamically assign column names for the DataFrame\n",
    "    columns = []\n",
    "    for cls in query_classes:\n",
    "        columns.append(f\"{cls}\")  # Adding columns for query_classes\n",
    "    for prop in query_properties:\n",
    "        columns.append(f\"{prop}\")  # Adding columns for query_properties    \n",
    "    \n",
    "    df = pd.DataFrame(events, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08452447-8d2c-4677-95e0-1ca1b18143cc",
   "metadata": {},
   "source": [
    "### 9. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68edee12-7fa4-48f6-aa3e-ea6d4dcf08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(text):\n",
    "    class_list, prop_list = bag_of_words()\n",
    "    text, tokens, clean_tokens, lemmatized_words =  clean_data(text)\n",
    "    display(Markdown('**1. Normalized output:**'))\n",
    "    print(text)\n",
    "    display(Markdown('**2. Tokenized Output:**'))\n",
    "    print(tokens)\n",
    "    display(Markdown('**3.Stopwords removed output:**'))\n",
    "    print(clean_tokens)\n",
    "    display(Markdown('**4. Lemmatized output:**'))\n",
    "    print(lemmatized_words)\n",
    "    ner_tokens, doc = ner(text, lemmatized_words)\n",
    "    query_classes, query_properties = ontology_map(ner_tokens, class_list, prop_list)\n",
    "    sparql_query = query_build(query_classes, query_properties, doc)\n",
    "    data = query_fetch(sparql_query)\n",
    "    df = create_dataframe(data, query_classes, query_properties)\n",
    "    display(Markdown('**5. NER output:**'))\n",
    "    print(ner_tokens)\n",
    "    display(Markdown('**6. Bag of Words Mapped output:**'))\n",
    "    print(query_classes, query_properties)\n",
    "    display(Markdown('**7. Query output:**'))\n",
    "    print(sparql_query)\n",
    "    display(Markdown('**8. Dataframe output:**'))\n",
    "    print(df)\n",
    "    # return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d80921b-dbba-4179-9b02-1bed1e4e5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SpeechRecognition pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1b456-3824-41e8-83d5-469ad44481aa",
   "metadata": {},
   "source": [
    "### 10.A) Audio Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84286c8a-485f-420c-9f27-f5e1589bac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something...\n",
      "Recognizing...\n",
      "Your Query:  disasters of China and people died\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1. Normalized output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disasters of China and people died\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2. Tokenized Output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disasters', 'of', 'China', 'and', 'people', 'died']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3.Stopwords removed output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disasters', 'China', 'people', 'died']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**4. Lemmatized output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disaster', 'China', 'people', 'die']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**5. NER output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disaster', 'China', 'people', 'die', 'gpe']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**6. Bag of Words Mapped output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eventName'] ['totalDeaths', 'countryName']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**7. Query output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT ?eventName ?totalDeaths ?countryName\n",
      "    WHERE {\n",
      "      ?event a <http://www.semanticweb.org/mohdtalhahussain/ontologies/2024/10/Disaster_Management_dataset#DisasterEvent>.\n",
      "    ?event <http://www.semanticweb.org/mohdtalhahussain/ontologies/2024/10/Disaster_Management_dataset#eventName> ?eventName.?event <http://www.semanticweb.org/mohdtalhahussain/ontologies/2024/10/Disaster_Management_dataset#totalDeaths> ?totalDeaths.\n",
      "?event <http://www.semanticweb.org/mohdtalhahussain/ontologies/2024/10/Disaster_Management_dataset#countryName> ?countryName.FILTER(?countryName = \"China\")\n",
      "    } LIMIT 20\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**8. Dataframe output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          eventName totalDeaths countryName\n",
      "0         Coal mine          19       China\n",
      "1             Cargo          11       China\n",
      "2  Chemical factory          78       China\n",
      "3  Chemical factory          10       China\n",
      "4          Building          10       China\n",
      "5             Cargo          10       China\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "# Initialize recognizer class (for recognizing the speech)\n",
    "recognizer = sr.Recognizer()\n",
    "#Microphone as the audio source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something...\")    \n",
    "    #Recognizer sensitivity\n",
    "    recognizer.adjust_for_ambient_noise(source, duration=1)    \n",
    "    #AudioData object\n",
    "    audio = recognizer.listen(source)    \n",
    "    print(\"Recognizing...\")   \n",
    "    #Google's speech recognition\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"Your Query: \", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "    df = process_data(text)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09747344-b418-4d43-9d7e-ec18a68eeef1",
   "metadata": {},
   "source": [
    "### 10.B) User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b56c2-4eff-4426-bba7-10992da8fd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Enter your question:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown('**Enter your question:**'))\n",
    "text = input()\n",
    "process_data(text)\n",
    "# df = process_data(text)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcc80b-a5fe-413b-abcc-d7244a519eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
